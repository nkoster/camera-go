<!DOCTYPE HTML>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<title>Camera</title>
</head>
<body>
Your ID is <b><span style="padding-right: 20px;" id="localid"></span></b> &nbsp; 
Remote ID is <b><input type="text" id="remoteid" /></b> &nbsp; 
<button id="start-button" style="margin-left: 40px;">Start</button>
<button id="stop-button">Stop</button><div id="div"></div>
<hr />
<video id="video" autoplay></video><audio id="audio"></audio>
<img id="receive" alt=""/>
<script>
(() => {
  const
    localID = document.getElementById('localid'),
    remoteID = document.getElementById('remoteid'),
    startButton = document.getElementById('start-button'),
    stopButton = document.getElementById('stop-button'),
    display = { width: 502, height: 376, quality: 0.3, fps: 16 }
    interval = 1000 / display.fps,
    proto = location.protocol === 'http:' ? 'ws' : 'wss',
    socketCam = new WebSocket(proto + '://' + location.host + '/cam'),
    socketMic = new WebSocket(proto + '://' + location.host + '/mic'),
    image = document.getElementById('receive'),
    video = document.getElementById('video'),
    audio = document.getElementById('audio'),
    canvas = document.createElement('canvas')

  let
    firstTime = true,
    playing = false

  socketMic.binaryType = 'arraybuffer'

  image.width = display.width
  image.height = display.height

  video.width = display.width
  video.height = display.height

  canvas.width = display.width
  canvas.height = display.height

  socketCam.onmessage = msg => {
    if (typeof msg.data === 'string') {
      localid.innerHTML = msg.data
      setTimeout(() => socketMic.send(msg.data), 100)
    } else {
      const blob = new Blob([msg.data], {type: 'image/jpeg'})
      image.src = URL.createObjectURL(blob)
    }
  }

  socketMic.onmessage = msg => {
    if (typeof msg.data === 'string') {
      console.log(msg)
      localid.innerHTML = msg.data
    } else {
      if (playing) {
        audioBuffer = audioCtx.createBuffer(1, 2048, 44100)
        audioBuffer.getChannelData(0).set(new Float32Array(msg.data))
        source = audioCtx.createBufferSource()
        source.buffer = audioBuffer
        source.connect(audioCtx.destination)
        source.start(0)
      }
    }
  }

  startButton.onclick = () => {
    if (!playing && remoteID.value.length > 2) {
      navigator.mediaDevices.getUserMedia({ video: true })
      .then(streamCam => {
        if (firstTime) {
          firstTime = false
          socketCam.send(remoteID.value)
          remoteID.outerHTML = '<span>' + remoteID.value + '<span>'
        }
        video.setAttribute('autoplay', '')
        video.setAttribute('muted', '')
        video.setAttribute('playsinline', '')
        video.srcObject = streamCam
        playing = true
        audioCtx = new AudioContext()
        track = audioCtx.createMediaElementSource(audio)
        track.connect(audioCtx.destination)
      })
      .catch(err => {
        console.log(err)
      })

      navigator.mediaDevices.getUserMedia({ audio: true })
      .then(streamMic => {
        const context = new AudioContext()
        const source = context.createMediaStreamSource(streamMic)
        const processor = context.createScriptProcessor(2048, 1, 1)
        source.connect(processor)
        processor.connect(context.destination)
        processor.onaudioprocess = evt => {
          playing && socketMic.send(new Float32Array(evt.inputBuffer.getChannelData(0)))
        }
      })
      .catch(err => {
        console.log(err)
      })
    }
  }

  stopButton.onclick = () => {
    video.srcObject &&
    video.srcObject.getTracks().forEach(track => {
      track.stop()
    })
    playing = false
  }

  const streamer = setInterval(() => {
    if (playing) {
      canvas.getContext('2d').drawImage(video, 0, 0, display.width, display.height)
      canvas.toBlob(blob => { socketCam.send(blob) }, 'image/jpeg', display.quality)
    }
  }, interval)

})()
</script>
</body>
</html>
